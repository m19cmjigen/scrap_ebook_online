# scrap_ebook_online

Ebook scraping and collection automation using Playwright for dynamic web content.

## Features

- ğŸ­ **Playwright-based scraping**: Handle JavaScript-rendered content
- âš¡ **Async/await support**: Concurrent scraping for better performance
- ğŸ”„ **Rate limiting**: Respectful scraping with configurable delays
- ğŸ” **Automatic retries**: Exponential backoff for transient failures
- ğŸ“ **Comprehensive logging**: Track all operations and errors
- ğŸ¨ **Extensible architecture**: Easy to add new scrapers and parsers
- ğŸ§ª **Type hints**: Full type annotation support
- ğŸ”’ **Ethical scraping**: Follows best practices and legal guidelines

## Quick Start

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd scrap_ebook_online
```

2. Create and activate a virtual environment:
```bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install Playwright browsers:
```bash
playwright install chromium
# Or install all browsers: playwright install
```

5. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your settings
```

## Usage

### Basic Example

```python
import asyncio
from src.scrapers.playwright_scraper import PlaywrightScraper

async def main():
    async with PlaywrightScraper(name="my_scraper", rate_limit=1.0) as scraper:
        # Scrape a single page
        data = await scraper.scrape("https://example.com")
        print(f"Title: {data['title']}")

asyncio.run(main())
```

### Custom Page Handler

```python
async def extract_book_info(page):
    """Extract specific information from a page."""
    return {
        'title': await page.locator('h1.title').text_content(),
        'author': await page.locator('.author').text_content(),
        'price': await page.locator('.price').text_content(),
    }

async with PlaywrightScraper() as scraper:
    data = await scraper.scrape(
        url="https://example.com/book/123",
        wait_for_selector='h1.title',
        custom_handler=extract_book_info
    )
```

### Multiple URLs

```python
urls = [
    "https://example.com/book/1",
    "https://example.com/book/2",
    "https://example.com/book/3",
]

async with PlaywrightScraper(rate_limit=2.0) as scraper:
    results = await scraper.scrape_multiple(
        urls,
        max_concurrent=3
    )
```

### Run Examples

```bash
python examples/basic_usage.py
```

## Project Structure

```
scrap_ebook_online/
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ scrapers/             # Scraper implementations
â”‚   â”‚   â”œâ”€â”€ base_scraper.py   # Base scraper class
â”‚   â”‚   â””â”€â”€ playwright_scraper.py  # Playwright scraper
â”‚   â”œâ”€â”€ parsers/              # HTML/data parsers
â”‚   â”œâ”€â”€ storage/              # Data storage handlers
â”‚   â”œâ”€â”€ utils/                # Utility functions
â”‚   â”‚   â”œâ”€â”€ logger.py         # Logging setup
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py   # Rate limiting
â”‚   â”‚   â””â”€â”€ retry.py          # Retry logic
â”‚   â””â”€â”€ config/               # Configuration
â”‚       â””â”€â”€ settings.py       # Settings loader
â”œâ”€â”€ examples/                 # Usage examples
â”‚   â””â”€â”€ basic_usage.py        # Basic examples
â”œâ”€â”€ tests/                    # Test files
â”‚   â”œâ”€â”€ unit/                 # Unit tests
â”‚   â””â”€â”€ integration/          # Integration tests
â”œâ”€â”€ data/                     # Output data (not tracked)
â”œâ”€â”€ logs/                     # Application logs (not tracked)
â”œâ”€â”€ CLAUDE.md                 # AI assistant guide
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ .env.example              # Example environment variables
```

## Configuration

Edit `.env` file to configure the scraper:

```bash
# Scraper settings
SCRAPER_RATE_LIMIT=1          # Requests per second
REQUEST_TIMEOUT=30            # Request timeout in seconds
MAX_RETRIES=3                 # Number of retry attempts

# Playwright settings
HEADLESS=true                 # Run browser in headless mode
BROWSER_TYPE=chromium         # chromium, firefox, or webkit
VIEWPORT_WIDTH=1920
VIEWPORT_HEIGHT=1080
NAVIGATION_TIMEOUT=30000      # Navigation timeout in milliseconds

# Logging
LOG_LEVEL=INFO                # DEBUG, INFO, WARNING, ERROR
```

## Development

### Running Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/unit/test_scrapers.py
```

### Code Formatting

```bash
# Format code with black
black src/ tests/ examples/

# Lint code
flake8 src/

# Type checking
mypy src/
```

### Adding a New Scraper

1. Create a new file in `src/scrapers/`
2. Inherit from `BaseScraper` or `PlaywrightScraper`
3. Implement required methods
4. Add tests in `tests/unit/`

Example:

```python
from src.scrapers.playwright_scraper import PlaywrightScraper

class CustomScraper(PlaywrightScraper):
    async def scrape_books(self, category: str):
        # Your custom implementation
        pass
```

## Legal and Ethical Considerations

âš ï¸ **Important**: Always scrape responsibly and ethically.

- âœ… Check and respect `robots.txt`
- âœ… Review website Terms of Service
- âœ… Implement appropriate rate limiting
- âœ… Use respectful User-Agent strings
- âœ… Only scrape publicly available data
- âœ… Consider the website's bandwidth costs
- âŒ Don't overwhelm servers with requests
- âŒ Don't scrape personal data without permission
- âŒ Don't violate copyright laws

See [CLAUDE.md](CLAUDE.md) for detailed guidelines.

## Troubleshooting

### Playwright Installation Issues

```bash
# Install browsers manually
playwright install chromium

# Install system dependencies (Linux)
playwright install-deps
```

### Import Errors

Make sure you're running from the project root or have the project in your PYTHONPATH:

```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Rate Limiting

If you get blocked, increase the rate limit delay:

```python
PlaywrightScraper(rate_limit=0.5)  # One request every 2 seconds
```

## Contributing

1. Read [CLAUDE.md](CLAUDE.md) for guidelines
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

[Add your license here]

## Resources

- [Playwright Documentation](https://playwright.dev/python/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Ethical Web Scraping Guide](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)

## Support

For issues and questions:
1. Check the documentation
2. Review [CLAUDE.md](CLAUDE.md)
3. Search existing issues
4. Create a new issue with details

---

**Remember**: Always scrape responsibly and respect website owners' resources and rights.
